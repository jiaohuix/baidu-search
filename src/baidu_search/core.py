"""
@date: 2026/02/13
@author: jiaohuix
@description: BaiduSearch - å¼‚æ­¥ç™¾åº¦æœç´¢æ¨¡å—

å·²å®ç°ï¼š
1. å¤šé¡µå¹¶å‘æœç´¢ï¼ˆasyncio + httpxï¼‰
2. å¹¶å‘æ§åˆ¶ï¼ˆSemaphore + QPS limiterï¼‰
3. æŠ–åŠ¨ + æŒ‡æ•°é€€é¿ + å…¨å±€å†·å´ï¼ˆæŠ—é£æ§ï¼‰
4. ç™¾åº¦ 302 è·³è½¬è§£æï¼ˆå¯å…³é—­ï¼‰
5. URL å»é‡ã€å™ªå£°è¿‡æ»¤ã€snipæå–ä¸æ¸…æ´—
6. æ‘˜è¦æå–ä¸æ¸…æ´—

TODOï¼š
1. ç½‘é¡µæ­£æ–‡æŠ“å–ï¼ˆè¿›å…¥çœŸå® URL æŠ“å– HTMLï¼Œä¸»å†…å®¹æå–ï¼‰
2. æŸ¥è¯¢ç»“æœç¼“å­˜ï¼ˆquery çº§ / url çº§ï¼Œæ”¯æŒ TTLï¼‰
3. å…³é”®ç‰‡æ®µæ‘˜å–ï¼ˆBM25 / è¯­ä¹‰é‡æ’ï¼‰
"""

import re
import asyncio
import logging
import random
import time
from enum import Enum
from urllib.parse import urlparse

import httpx
from aiolimiter import AsyncLimiter
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)

NOISE_PATTERNS  = r"é«˜æ¸…è§†é¢‘|åœ¨çº¿è§‚çœ‹|å®æ—¶å›å¤|ç²¾é€‰ç¬”è®°|æ·˜å®"
BANED_SITES = ["www.taobao.com"]

class UrlResolveStatus(str, Enum):
    SKIPPED = "skipped"      # ä¸éœ€è¦è§£æ
    RESOLVED = "resolved"    # æˆåŠŸæ‹¿åˆ° Location
    FAILED = "failed"        # å°è¯•äº†ä½†å¤±è´¥


class ContentFilter:
    def __init__(self, banned_sites=None, noise_patterns=None):
        self.banned_sites = banned_sites or []
        self.re_noise = re.compile(noise_patterns) if noise_patterns else None

    def _is_banned_site(self, url: str) -> bool:
        netloc = urlparse(url).netloc
        return any(site in netloc for site in self.banned_sites)

    def filter_results(self, results: list[dict], limit: int) -> list[dict]:
        """Filter search results by URL validity, duplicates, banned sites, and noise."""
        res = []
        seen_urls = set()

        for result in results:
            url = result.get("url") or ""
            title = result.get("title", "")
            abstract = result.get("abstract", "")

            # åˆå¹¶æ‰€æœ‰è·³è¿‡æ¡ä»¶
            if (
                not url.startswith("http") or
                url in seen_urls or
                self._is_banned_site(url) or
                (self.re_noise and (self.re_noise.search(title) or self.re_noise.search(abstract)))
            ):
                continue

            seen_urls.add(url)
            result["rank"] = len(res) + 1
            res.append(result)

            if len(res) >= limit:
                break

        return res


# â”€â”€ é»˜è®¤å¹¶å‘é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å¯é€šè¿‡ config["concurrency"] è¦†ç›–ï¼Œæ–¹ä¾¿è°ƒè¯•
DEFAULT_CONCURRENCY = {
    # ç™¾åº¦æœç´¢é¡µ
    "search_sem": 2,          # åŒæ—¶æœ€å¤šå‡ ä¸ªæœç´¢é¡µåœ¨é£
    "search_qps": 0.5,          # æ¯ç§’æœ€å¤šå‘å‡ ä¸ªæœç´¢é¡µè¯·æ±‚
    "search_jitter": (0.05, 0.15),  # æœç´¢é¡µè¯·æ±‚å‰çš„éšæœºæŠ–åŠ¨(ç§’)
    # link è§£æ (302)ï¼šè½»é‡ HEAD
    "resolve_sem": 15,        # åŒæ—¶æœ€å¤šå‡ ä¸ªè§£æåœ¨é£ ã€é€Ÿåº¦ç“¶é¢ˆåœ¨urlè§£æè¿™ã€‘
    "resolve_qps": 10,        # æ¯ç§’æœ€å¤šå‘å‡ ä¸ªè§£æè¯·æ±‚ resolve_qps = min(10, search_qps * 10)
    "resolve_jitter": (0.02, 0.08), # URL è§£æè¯·æ±‚å‰çš„éšæœºæŠ–åŠ¨(ç§’)
    # é‡è¯•ï¼ˆä»…æœç´¢é¡µï¼‰
    "max_retries": 2,
    "retry_backoff": 3.0,
    "resolve_real_url": True,
    # "resolve_real_url": False,
}


class BaiduSearch:
    """ç™¾åº¦æœç´¢ + sem/qps ä¿æŠ¤ã€‚"""
    # å›ºå®š headersï¼Œè·Ÿ core.py ä¿æŒä¸€è‡´ï¼ˆåŒè¿æ¥å†… UA ä¸å˜ï¼Œæ›´åƒçœŸå®æµè§ˆå™¨ï¼‰
    _HEADERS = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                      "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;"
                  "q=0.9,image/webp,*/*;q=0.8",
        "Referer": "https://www.baidu.com/",
    }

    def __init__(self, config: dict = None) -> None:
        self.url = "https://www.baidu.com/s"
        config = config or {}
        search_banned_sites = config.get("search_banned_sites", [])
        search_noise_patterns = config.get("search_noise_patterns", "")
        self.content_filter = ContentFilter(search_banned_sites, search_noise_patterns)
        self.max_results = config.get("max_results", 100)

        # â”€â”€ å¹¶å‘å‚æ•°ï¼ˆå¯é€šè¿‡ config["concurrency"] è¦†ç›–ï¼‰ â”€â”€
        cc = {**DEFAULT_CONCURRENCY, **config.get("concurrency", {})}
        self._cc = cc
        # æœç´¢é¡µå¹¶å‘æ§åˆ¶
        self._search_sem = asyncio.Semaphore(cc["search_sem"])
        self._search_qps = self._make_limiter(cc["search_qps"])
        # link è§£æå¹¶å‘æ§åˆ¶
        self._resolve_sem = asyncio.Semaphore(cc["resolve_sem"])
        self._resolve_qps = self._make_limiter(cc["resolve_qps"])
        self._cooldown_until = 0
        # æ˜¯å¦è§£æçœŸå®url
        self.resolve_real_url = cc.get("resolve_real_url", True)

    @staticmethod
    def _make_limiter(qps: float) -> AsyncLimiter:
        """æ„é€  AsyncLimiterï¼Œç¡®ä¿ max_rate >= 1 ä»¥é¿å… acquire æŠ¥é”™ã€‚
        ä¾‹å¦‚ qps=0.33 â†’ AsyncLimiter(1, 1/0.33â‰ˆ3.03)ï¼Œå³ 3 ç§’ 1 æ¬¡ã€‚
        """
        if qps >= 1:
            return AsyncLimiter(qps, 1)
        else:
            # åè½¬ï¼š1 æ¬¡ / (1/qps) ç§’
            return AsyncLimiter(1, 1.0 / qps)

    async def search(self, query: str, num_results: int = 5) -> str:
        """æœç´¢ç™¾åº¦å¹¶è¿”å›ç»“æœ"""
        res = await self.search_baidu(query, num_results=num_results)

        # filter
        if self.content_filter:
            results = self.content_filter.filter_results(res["data"], num_results)
        else:
            results = res["data"][:num_results]

        # format
        formatted_results = []
        for i, r in enumerate(results, 1):
            formatted_results.append(f"{i}. {r['title']} ({r['url']})")
            if "abstract" in r:
                formatted_results[-1] += f"\nAbstract: {r['abstract']}"

        msg = "\n".join(formatted_results)
        return msg


    async def search_baidu(self, query, num_results=10):
        """ç™¾åº¦æœç´¢ä¸»æµç¨‹ã€‚
        æ€è·¯ï¼šsem + qps ä¸¤å±‚æ§åˆ¶å³å¯ï¼Œä½é¢‘è°ƒç”¨é›¶ç­‰å¾…ï¼Œé«˜é¢‘è‡ªåŠ¨æ’é˜Ÿã€‚
        """
        pages_needed = (num_results + 9) // 10
        t0 = time.time()

        # http2=True + å›ºå®š headers åœ¨ client çº§åˆ«
        async with httpx.AsyncClient(headers=self._HEADERS, http2=True) as client:
            t1 = time.time()
            logger.info(f"[è®¡æ—¶] åˆå§‹åŒ– {t1-t0:.2f}s")

            # â‘¡ æœç´¢é¡µï¼šgather å¹¶å‘ï¼Œsem + qps è‡ªåŠ¨é™é€Ÿ
            results = await self._fetch_pages_concurrent(client, query, pages_needed)
            t2 = time.time()
            logger.info(f"[è®¡æ—¶] æœç´¢é¡µ {pages_needed} é¡µ â†’ {len(results)} æ¡ï¼Œ{t2-t1:.2f}s")

            # â‘¢ link è§£æï¼šgather å¹¶å‘ï¼Œsem + qps è‡ªåŠ¨é™é€Ÿ
            if self.resolve_real_url:
                await self._resolve_urls_concurrent(client, results)
                t3 = time.time()
                logger.info(f"[è®¡æ—¶] URL è§£æ {len(results)} æ¡ï¼Œ{t3-t2:.2f}s")
            else:
                # æ ‡è®°ä¸ºè·³è¿‡è§£æ
                for item in results:
                    item["url_status"] = UrlResolveStatus.SKIPPED.value
                t3 = time.time()
                logger.info(f"[è®¡æ—¶] URL è§£æå·²å…³é—­")

            # â‘£ é™çº§ä¿ç•™
            cleaned = [
                item for item in results
                if item.get("url_status") != UrlResolveStatus.FAILED.value
                or item.get("url", "").startswith("http")
            ]
            if not cleaned:
                logger.warning(f"URL å…¨éƒ¨è§£æå¤±è´¥: {query}ï¼Œè¿”å›åŸå§‹ç»“æœ")
                cleaned = results

            logger.info(f"[è®¡æ—¶] æ€»è€—æ—¶ {t3-t0:.2f}sï¼Œè¿”å› {len(cleaned)} æ¡")
            return {"data": cleaned}

    # â”€â”€ æœç´¢é¡µï¼šå¹¶å‘æŠ“å– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    async def _fetch_pages_concurrent(self, client, query, pages_needed):
        """æ‰€æœ‰é¡µ gather å¹¶å‘ï¼Œç”± sem + qps è‡ªåŠ¨æ§åˆ¶èŠ‚å¥ã€‚"""
        tasks = [
            asyncio.create_task(self._fetch_page_throttled(client, query, i))
            for i in range(pages_needed)
        ]
        pages = await asyncio.gather(*tasks)
        # åˆå¹¶ç»“æœï¼Œè·³è¿‡è¢«æ‹¦æˆªçš„é¡µï¼ˆNoneï¼‰
        results = []
        for page in pages:
            if page is not None:
                results.extend(page)
        return results

    async def _fetch_page_throttled(self, client, query, page_idx):
        """å•é¡µè¯·æ±‚ï¼šæŠ–åŠ¨ + sem + qps é™é€Ÿï¼Œè¢«æ‹¦æˆªæ—¶ backoff é‡è¯•ã€‚"""
        max_retries = self._cc["max_retries"]
        backoff = self._cc["retry_backoff"]
        jitter = self._cc["search_jitter"]

        for attempt in range(1 + max_retries):

            # ğŸ‘‡ æ¯æ¬¡å°è¯•å‰æ£€æŸ¥å†·å´
            now = time.time()
            wait = max(0, self._cooldown_until - now)
            if wait > 0:
                logger.warning(f"å…¨å±€å†·å´ä¸­ï¼Œç­‰å¾… {wait:.1f}s")
                await asyncio.sleep(wait)

            # æŠ–åŠ¨ï¼šè®©åŒæ‰¹ task é”™å¼€åˆ°è¾¾
            await asyncio.sleep(random.uniform(*jitter))
            async with self._search_qps:
                async with self._search_sem:
                    data = await self.fetch_page(client, query, page_idx)
            if data is not None:
                return data
            # è¢«æ‹¦æˆªï¼Œbackoff é‡è¯•
            if attempt < max_retries:
                # wait = backoff * (attempt + 1) # çº¿æ€§é€€é¿
                wait = backoff * (2 ** attempt) # æŒ‡æ•°é€€é¿
                logger.warning(
                    f"æœç´¢é¡µ {page_idx} è¢«æ‹¦æˆªï¼Œ{wait:.1f}s åé‡è¯• "
                    f"({attempt+1}/{max_retries})"
                )
                await asyncio.sleep(wait)
        return None  # é‡è¯•è€—å°½

    # â”€â”€ link è§£æï¼šå¹¶å‘è§£æ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    async def _resolve_urls_concurrent(self, client, results):
        """æ‰€æœ‰ URL gather å¹¶å‘ï¼Œç”± sem + qps è‡ªåŠ¨æ§åˆ¶èŠ‚å¥ã€‚"""
        tasks = [
            asyncio.create_task(self._resolve_one(client, item))
            for item in results
        ]
        if tasks:
            await asyncio.gather(*tasks)

    async def _resolve_one(self, client, item):
        """å•æ¡ URL è§£æï¼šæŠ–åŠ¨ + sem + qps é™é€Ÿï¼Œä¸é‡è¯•ã€‚"""
        jitter = self._cc["resolve_jitter"]
        # æŠ–åŠ¨ï¼šè®©åŒæ‰¹ task é”™å¼€åˆ°è¾¾
        await asyncio.sleep(random.uniform(*jitter))
        async with self._resolve_qps:
            async with self._resolve_sem:
                url, status = await self.get_real_url(client, item["url"])
        item["url"] = url
        item["url_status"] = status.value


    async def get_real_url(self, client, url):
        """è§£æç™¾åº¦è·³è½¬é“¾æ¥ï¼Œè·å–çœŸå® URLï¼ˆçº¯é€»è¾‘ï¼Œä¸å«é™é€Ÿï¼‰ã€‚"""
        if not url:
            return url, UrlResolveStatus.SKIPPED

        # éè·³è½¬ URL ç›´æ¥è·³è¿‡
        if not ("link?url=" in url or "baidu.php" in url):
            return url, UrlResolveStatus.SKIPPED

        try:
            resp = await client.head(
                url, follow_redirects=False, timeout=2.0,
            )
            location = resp.headers.get("Location")
            if location:
                return location, UrlResolveStatus.RESOLVED
            return url, UrlResolveStatus.FAILED
        except Exception as e:
            logger.exception(f"fetch_page å¼‚å¸¸: {e}")
            return url, UrlResolveStatus.FAILED

    def clean_abstract(self, text):
        """æ¸…æ´—ä¹±ç å’Œå†—ä½™æ¢è¡Œ"""
        if not text: return ""
        
        # 1. å»æ‰ç‰¹æ®Šçš„ç¼–ç å­—ç¬¦ï¼ˆå¦‚ \ue680, \ue67d ç­‰ç™¾åº¦å›¾æ ‡å­—ä½“ï¼‰
        text = re.sub(r'[\ue600-\ue6ff]', '', text)
        
        # 2. å°†å¤šä¸ªæ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦ç»Ÿä¸€æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼ï¼Œä¿æŒç»“æ„ç´§å‡‘
        text = re.sub(r'[\n\t\r]+', ' ', text)
        
        # 3. å»æ‰çº¯ç²¹çš„äº¤äº’è¯å™ªå£°ï¼ˆå¦‚â€œæ’­æŠ¥â€ã€â€œæš‚åœâ€ã€â€œç‚¹å‡»æŸ¥çœ‹â€ï¼‰
        noise = ["æ’­æŠ¥", "æš‚åœ", "æŸ¥çœ‹æ›´å¤š", "å±•å¼€å…¨éƒ¨"]
        for n in noise:
            text = text.replace(n, "")
        
        # 4. å»é™¤é¦–å°¾åŠä¸­é—´å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text).strip()
        return text


    def extract_abstract(self, container):
        """ä»å®¹å™¨ä¸­æå–æ‘˜è¦æ–‡æœ¬å—"""
        # å°è¯•ç™¾åº¦æœ€å¸¸ç”¨çš„å‡ ä¸ªå†…å®¹ç±»å
        selectors = [".c-abstract", ".content-right_8Zs4j", ".content-abstract", ".op-se-share-content",".c-span-last"]

        for s in selectors:
            node = container.select_one(s)
            if node: return node.get_text()

        # å…œåº•ï¼šå¦‚æœæ‰¾ä¸åˆ°æŒ‡å®šç±»ï¼Œå°±æ‰¾åŒ…å«æ–‡æœ¬æœ€å¤šçš„å­å—
        child_nodes = container.find_all(["div", "span"])
        if child_nodes:
            # è¿‡æ»¤æ‰å­—æ•°å¤ªå°‘çš„ï¼ˆæ¯”å¦‚åªæœ‰â€œå¹¿å‘Šâ€ä¸¤ä¸ªå­—çš„ï¼‰
            texts = [t.get_text().strip() for t in child_nodes if len(t.get_text().strip()) > 20]
            if texts:
                return max(texts, key=len)
        return ""

    async def fetch_page(self, client, keyword, page_idx):
        """å•é¡µè¯·æ±‚ï¼ˆçº¯é€»è¾‘ï¼Œä¸å«é™é€Ÿï¼‰ã€‚è¿”å› None è¡¨ç¤ºè¢«æ‹¦æˆªï¼Œ[] è¡¨ç¤ºè§£æå¼‚å¸¸ã€‚"""

        params =    {
            "wd": keyword,
            "pn": page_idx * 10,
            "ie": "utf-8",
        }
        try:
            resp = await client.get(self.url, params=params, timeout=5.0)

            # æ£€æµ‹éªŒè¯ç æ‹¦æˆª â†’ è¿”å› None è§¦å‘ä¸Šå±‚é‡è¯•
            if "ç™¾åº¦å®‰å…¨éªŒè¯" in resp.text:
                logger.warning(f"è§¦å‘ç™¾åº¦å®‰å…¨éªŒè¯ï¼Œç¬¬ {page_idx} é¡µ")

                # è®¾ç½®å…¨å±€å†·å´ 30 ç§’
                self._cooldown_until = time.time() + 30
                return None

            soup = BeautifulSoup(resp.text, "lxml")
            containers = soup.select(".c-container")
            
            page_items = []
            for i, container in enumerate(containers):
                title_node = container.select_one("h3") or container.select_one(".t")
                if not title_node: continue
                
                title = title_node.get_text(strip=True)
                raw_url = title_node.find("a")["href"] if title_node.find("a") else ""
                
                # æå–å¹¶æ¸…æ´—æ‘˜è¦
                raw_abstract = self.extract_abstract(container)
                clean_abs = self.clean_abstract(raw_abstract)

                page_items.append({
                    "rank": page_idx * 10 + i + 1,
                    "title": title,
                    "abstract": clean_abs,
                    "url": raw_url
                })
            return page_items
        except Exception as e:
            logger.exception(f"fetch_page å¼‚å¸¸: {e}")
            return []
    


async def main():

    config = {
        "search_noise_patterns": NOISE_PATTERNS,
        "search_banned_sites": BANED_SITES,
        "concurrency": {
            # æœç´¢é¡µ
            "search_sem": 2,
            "search_qps": 0.5,
            "search_jitter": (0.05, 0.15),

            # URL è§£æ
            "resolve_sem": 15,
            "resolve_qps": 10,
            "resolve_jitter": (0.02, 0.08),

            # é‡è¯•
            "max_retries": 2,
            "retry_backoff": 3.0,

            # æ˜¯å¦è§£æçœŸå® URL
            "resolve_real_url": True,
        }
    }
    searcher = BaiduSearch(config)
    keyword = "å¼ºåŒ–å­¦ä¹ "
    print(f"å¼€å§‹æŠ“å–å…³é”®è¯: {keyword} ...")
    # results = await searcher.search(keyword, num_results=10)
    # print(results)
    
    results = await searcher.search_baidu(keyword, num_results=10)
    for item in results["data"]:
        print(f"[{item['rank']}] {item['title']}")
        print(f"æ¥æº/åœ°å€: {item['url']}")
        print(f"å†…å®¹æ‘˜è¦: {item['abstract']}")
        print("-" * 40)



if __name__ == "__main__":
    asyncio.run(main())
   